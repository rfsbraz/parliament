#!/usr/bin/env python3
"""
Batch Analytics Processor

This script is designed to be run after large data imports or as a scheduled job.
It processes all analytics efficiently and provides detailed reporting on the results.

Usage:
    python batch_analytics_processor.py                          # Process all data
    python batch_analytics_processor.py --legislature 15        # Process specific legislature
    python batch_analytics_processor.py --schedule daily        # Scheduled daily run
    python batch_analytics_processor.py --report-only           # Generate reports without processing
"""

import sys
import os
import argparse
import json
from datetime import datetime, date
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from sqlalchemy import create_engine, text, func
from sqlalchemy.orm import sessionmaker
from database.models import (
    DeputyAnalytics, AttendanceAnalytics, InitiativeAnalytics,
    DeputyTimeline, DataQualityMetrics, Deputado, Legislatura
)
from database.connection import get_engine


@dataclass
class ProcessingStats:
    """Statistics for analytics processing run."""
    start_time: datetime
    end_time: Optional[datetime] = None
    legislatures_processed: int = 0
    deputies_processed: int = 0
    analytics_records_created: int = 0
    analytics_records_updated: int = 0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    @property
    def duration(self) -> str:
        if self.end_time:
            delta = self.end_time - self.start_time
            return str(delta)
        return "In progress..."
    
    @property
    def success_rate(self) -> float:
        if self.deputies_processed == 0:
            return 0.0
        return ((self.deputies_processed - len(self.errors)) / self.deputies_processed) * 100


class BatchAnalyticsProcessor:
    """
    Batch processor for Portuguese Parliamentary analytics.
    
    Handles large-scale analytics processing with progress tracking,
    error handling, and comprehensive reporting.
    """
    
    def __init__(self, engine, verbose: bool = False):
        self.engine = engine
        self.Session = sessionmaker(bind=engine)
        self.verbose = verbose
        self.stats = ProcessingStats(start_time=datetime.now())
    
    def process_all_analytics(self, legislatura_id: Optional[int] = None, report_only: bool = False):
        """
        Process all analytics with comprehensive tracking and reporting.
        
        Args:
            legislatura_id: Optional legislature ID to filter processing
            report_only: If True, only generate reports without processing
        """
        print("Portuguese Parliament Analytics Batch Processor")
        print("=" * 60)
        
        if report_only:
            self._generate_analytics_report(legislatura_id)
            return
        
        # Get legislatures to process
        if legislatura_id:
            legislatures = [legislatura_id]
            print(f" Processing Legislature {legislatura_id}")
        else:
            legislatures = self._get_all_legislatures()
            print(f" Processing {len(legislatures)} Legislatures: {legislatures}")
        
        print(f" Started at: {self.stats.start_time}")
        print()
        
        # Process each legislature
        for legislature in legislatures:
            self._process_legislature(legislature)
        
        # Finalize stats
        self.stats.end_time = datetime.now()
        self.stats.legislatures_processed = len(legislatures)
        
        # Generate final report
        self._print_final_report()
        
        # Save processing log
        self._save_processing_log()
    
    def _get_all_legislatures(self) -> List[int]:
        """Get all available legislature IDs."""
        with self.Session() as session:
            result = session.query(Legislatura.id).order_by(Legislatura.id.desc()).all()
            return [r[0] for r in result]
    
    def _process_legislature(self, legislatura_id: int):
        """Process all analytics for a specific legislature."""
        print(f"  Processing Legislature {legislatura_id}")
        print("-" * 40)
        
        with self.Session() as session:
            # Get deputies count
            deputies_count = session.query(func.count(Deputado.id)).filter(
                Deputado.legislatura_id == legislatura_id
            ).scalar()
            
            print(f" Found {deputies_count} deputies")
            
            if deputies_count == 0:
                print("WARNING:  No deputies found, skipping...")
                print()
                return
            
            # Process each analytics component
            components = [
                ("Deputy Analytics", self._process_deputy_analytics_batch),
                ("Attendance Analytics", self._process_attendance_analytics_batch),
                ("Initiative Analytics", self._process_initiative_analytics_batch),
                ("Deputy Timelines", self._process_timeline_analytics_batch),
                ("Data Quality Metrics", self._process_quality_metrics_batch)
            ]
            
            for component_name, processor_func in components:
                try:
                    print(f"  Processing {component_name}...")
                    created, updated = processor_func(legislatura_id)
                    self.stats.analytics_records_created += created
                    self.stats.analytics_records_updated += updated
                    print(f"    {component_name}: {created} created, {updated} updated")
                except Exception as e:
                    error_msg = f"Legislature {legislatura_id} - {component_name}: {str(e)}"
                    self.stats.errors.append(error_msg)
                    print(f"    {component_name}: Error - {str(e)}")
            
            self.stats.deputies_processed += deputies_count
            print(f" Legislature {legislatura_id} completed")
            print()
    
    def _process_deputy_analytics_batch(self, legislatura_id: int) -> tuple[int, int]:
        """Process deputy analytics in batch mode using pure Python calculations."""
        try:
            # Import the analytics calculator
            from scripts.analytics.calculate_analytics import AnalyticsCalculator
            
            # Create calculator and process entire legislature
            calculator = AnalyticsCalculator(self.engine, verbose=self.verbose)
            
            # Get initial count
            with self.Session() as session:
                initial_count = session.query(DeputyAnalytics).filter(
                    DeputyAnalytics.legislatura_id == legislatura_id
                ).count()
            
            # Process all deputy analytics for this legislature
            calculator.calculate_all_analytics(legislatura_id=legislatura_id, force_refresh=True)
            
            # Get final count
            with self.Session() as session:
                final_count = session.query(DeputyAnalytics).filter(
                    DeputyAnalytics.legislatura_id == legislatura_id
                ).count()
                
                deputy_count = session.query(Deputado.id).filter(
                    Deputado.legislatura_id == legislatura_id
                ).count()
            
            # Calculate created vs updated (simplified)
            created = final_count - initial_count if final_count > initial_count else 0
            updated = deputy_count - created
            
            return created, updated
            
        except Exception as e:
            self.stats.errors.append(f"Deputy analytics batch processing: {str(e)}")
            return 0, 0
    
    def _process_attendance_analytics_batch(self, legislatura_id: int) -> tuple[int, int]:
        """Process attendance analytics in batch mode."""
        created, updated = 0, 0
        
        with self.Session() as session:
            # Get all deputy-month combinations that need processing
            monthly_data = session.execute(text("""
                SELECT DISTINCT 
                    d.id as deputy_id,
                    YEAR(ma.dt_reuniao) as year,
                    MONTH(ma.dt_reuniao) as month
                FROM deputados d
                JOIN meeting_attendances ma ON d.id = ma.dep_id
                WHERE d.legislatura_id = :leg_id
                ORDER BY d.id, year, month
            """), {"leg_id": legislatura_id}).fetchall()
            
            for deputy_id, year, month in monthly_data:
                try:
                    # Check if record exists
                    existing = session.query(AttendanceAnalytics).filter(
                        AttendanceAnalytics.deputado_id == deputy_id,
                        AttendanceAnalytics.legislatura_id == legislatura_id,
                        AttendanceAnalytics.year == year,
                        AttendanceAnalytics.month == month
                    ).first()
                    
                    # Calculate monthly statistics
                    monthly_stats = session.execute(text("""
                        SELECT 
                            COUNT(*) as scheduled,
                            SUM(CASE WHEN presenca = 'P' THEN 1 ELSE 0 END) as attended
                        FROM meeting_attendances ma
                        WHERE ma.dep_id = :deputy_id
                        AND YEAR(ma.dt_reuniao) = :year
                        AND MONTH(ma.dt_reuniao) = :month
                    """), {
                        "deputy_id": deputy_id,
                        "year": year,
                        "month": month
                    }).fetchone()
                    
                    scheduled = monthly_stats.scheduled
                    attended = monthly_stats.attended
                    attendance_rate = (attended / scheduled * 100) if scheduled > 0 else 0
                    
                    if existing:
                        # Update existing
                        existing.sessions_scheduled = scheduled
                        existing.sessions_attended = attended
                        existing.sessions_absent = scheduled - attended
                        existing.attendance_rate = attendance_rate
                        existing.updated_at = datetime.now()
                        updated += 1
                    else:
                        # Create new
                        analytics = AttendanceAnalytics(
                            deputado_id=deputy_id,
                            legislatura_id=legislatura_id,
                            year=year,
                            month=month,
                            sessions_scheduled=scheduled,
                            sessions_attended=attended,
                            sessions_absent=scheduled - attended,
                            attendance_rate=attendance_rate
                        )
                        session.add(analytics)
                        created += 1
                        
                except Exception as e:
                    self.stats.errors.append(f"Attendance Deputy {deputy_id} {year}-{month}: {str(e)}")
            
            session.commit()
        
        return created, updated
    
    def _process_initiative_analytics_batch(self, legislatura_id: int) -> tuple[int, int]:
        """Process initiative analytics in batch mode."""
        created, updated = 0, 0
        
        with self.Session() as session:
            deputies = session.query(Deputado.id).filter(
                Deputado.legislatura_id == legislatura_id
            ).all()
            
            for deputy_id, in deputies:
                try:
                    # Check if record exists
                    existing = session.query(InitiativeAnalytics).filter(
                        InitiativeAnalytics.deputado_id == deputy_id,
                        InitiativeAnalytics.legislatura_id == legislatura_id
                    ).first()
                    
                    # Get initiative statistics
                    stats = session.execute(text("""
                        SELECT 
                            COUNT(DISTINCT i.id) as total_authored,
                            SUM(CASE WHEN i.estado = 'Aprovado' THEN 1 ELSE 0 END) as approved,
                            SUM(CASE WHEN i.estado = 'Em curso' THEN 1 ELSE 0 END) as in_progress,
                            SUM(CASE WHEN i.estado = 'Rejeitado' THEN 1 ELSE 0 END) as rejected,
                            MIN(i.data) as first_date,
                            MAX(i.data) as latest_date
                        FROM iniciativas i
                        JOIN iniciativas_autores ia ON i.id = ia.iniciativa_id
                        WHERE ia.autor_deputado_id = :deputy_id
                        AND i.legislatura_id = :leg_id
                    """), {
                        "deputy_id": deputy_id,
                        "leg_id": legislatura_id
                    }).fetchone()
                    
                    total_authored = stats.total_authored or 0
                    approved = stats.approved or 0
                    in_progress = stats.in_progress or 0
                    rejected = stats.rejected or 0
                    first_date = stats.first_date
                    latest_date = stats.latest_date
                    
                    success_rate = (approved / total_authored * 100) if total_authored > 0 else 0
                    
                    if existing:
                        # Update existing
                        existing.total_initiatives_authored = total_authored
                        existing.initiatives_approved = approved
                        existing.initiatives_in_progress = in_progress
                        existing.initiatives_rejected = rejected
                        existing.success_rate = success_rate
                        existing.first_initiative_date = first_date
                        existing.latest_initiative_date = latest_date
                        existing.updated_at = datetime.now()
                        updated += 1
                    else:
                        # Create new
                        analytics = InitiativeAnalytics(
                            deputado_id=deputy_id,
                            legislatura_id=legislatura_id,
                            total_initiatives_authored=total_authored,
                            initiatives_approved=approved,
                            initiatives_in_progress=in_progress,
                            initiatives_rejected=rejected,
                            success_rate=success_rate,
                            first_initiative_date=first_date,
                            latest_initiative_date=latest_date
                        )
                        session.add(analytics)
                        created += 1
                        
                except Exception as e:
                    self.stats.errors.append(f"Initiative Deputy {deputy_id}: {str(e)}")
            
            session.commit()
        
        return created, updated
    
    def _process_timeline_analytics_batch(self, legislatura_id: int) -> tuple[int, int]:
        """Process deputy timeline analytics in batch mode."""
        created, updated = 0, 0
        
        with self.Session() as session:
            deputies = session.query(Deputado.id).filter(
                Deputado.legislatura_id == legislatura_id
            ).all()
            
            for deputy_id, in deputies:
                try:
                    # Check if record exists (timeline is per deputy, not per legislature)
                    existing = session.query(DeputyTimeline).filter(
                        DeputyTimeline.deputado_id == deputy_id
                    ).first()
                    
                    # Get career statistics
                    career_stats = session.execute(text("""
                        SELECT 
                            COUNT(DISTINCT d.legislatura_id) as total_legislatures,
                            MIN(l.data_inicio) as first_election,
                            MAX(l.data_fim) as current_term_end
                        FROM deputados d
                        JOIN legislaturas l ON d.legislatura_id = l.id
                        WHERE d.id = :deputy_id
                    """), {"deputy_id": deputy_id}).fetchone()
                    
                    total_legislatures = career_stats.total_legislatures or 0
                    first_election = career_stats.first_election
                    current_term_end = career_stats.current_term_end
                    
                    # Calculate years of service
                    if first_election and current_term_end:
                        years_of_service = (current_term_end - first_election).days // 365
                    else:
                        years_of_service = 0
                    
                    # Determine experience category
                    if years_of_service <= 2:
                        experience_category = 'junior'
                    elif years_of_service <= 6:
                        experience_category = 'mid-career'
                    elif years_of_service <= 12:
                        experience_category = 'senior'
                    else:
                        experience_category = 'veteran'
                    
                    if existing:
                        # Update existing
                        existing.first_election_date = first_election
                        existing.total_legislatures_served = total_legislatures
                        existing.years_of_service = years_of_service
                        existing.experience_category = experience_category
                        existing.updated_at = datetime.now()
                        updated += 1
                    else:
                        # Create new
                        timeline = DeputyTimeline(
                            deputado_id=deputy_id,
                            first_election_date=first_election,
                            total_legislatures_served=total_legislatures,
                            years_of_service=years_of_service,
                            experience_category=experience_category
                        )
                        session.add(timeline)
                        created += 1
                        
                except Exception as e:
                    self.stats.errors.append(f"Timeline Deputy {deputy_id}: {str(e)}")
            
            session.commit()
        
        return created, updated
    
    def _process_quality_metrics_batch(self, legislatura_id: int) -> tuple[int, int]:
        """Process data quality metrics in batch mode."""
        created, updated = 0, 0
        today = date.today()
        
        tables_to_check = [
            'deputados', 'meeting_attendances', 'iniciativas', 'intervencoes'
        ]
        
        with self.Session() as session:
            for table_name in tables_to_check:
                try:
                    # Check if record exists for today
                    existing = session.query(DataQualityMetrics).filter(
                        DataQualityMetrics.table_name == table_name,
                        DataQualityMetrics.metric_date == today
                    ).first()
                    
                    if existing:
                        continue  # Skip if already calculated today
                    
                    # Get table statistics (simplified)
                    if table_name == 'deputados':
                        total_records = session.execute(text("""
                            SELECT COUNT(*) FROM deputados WHERE legislatura_id = :leg_id
                        """), {"leg_id": legislatura_id}).scalar()
                    elif table_name == 'meeting_attendances':
                        total_records = session.execute(text("""
                            SELECT COUNT(*) FROM meeting_attendances ma
                            JOIN deputados d ON ma.dep_id = d.id
                            WHERE d.legislatura_id = :leg_id
                        """), {"leg_id": legislatura_id}).scalar()
                    elif table_name == 'iniciativas':
                        total_records = session.execute(text("""
                            SELECT COUNT(*) FROM iniciativas WHERE legislatura_id = :leg_id
                        """), {"leg_id": legislatura_id}).scalar()
                    elif table_name == 'intervencoes':
                        total_records = session.execute(text("""
                            SELECT COUNT(*) FROM intervencoes WHERE legislatura_id = :leg_id
                        """), {"leg_id": legislatura_id}).scalar()
                    else:
                        total_records = 0
                    
                    # Simplified quality metrics (could be enhanced)
                    complete_records = int(total_records * 0.95)  # Assume 95% completeness
                    incomplete_records = total_records - complete_records
                    completeness_percentage = 95.0 if total_records > 0 else 0.0
                    
                    # Create quality metrics record
                    quality_metrics = DataQualityMetrics(
                        table_name=table_name,
                        metric_date=today,
                        total_records=total_records,
                        complete_records=complete_records,
                        incomplete_records=incomplete_records,
                        completeness_percentage=completeness_percentage,
                        consistency_score=90,
                        referential_integrity_score=95,
                        temporal_consistency_score=85,
                        quality_trend='stable'
                    )
                    session.add(quality_metrics)
                    created += 1
                    
                except Exception as e:
                    self.stats.errors.append(f"Quality metrics {table_name}: {str(e)}")
            
            session.commit()
        
        return created, updated
    
    def _generate_analytics_report(self, legislatura_id: Optional[int] = None):
        """Generate comprehensive analytics report."""
        print(" PORTUGUESE PARLIAMENT ANALYTICS REPORT")
        print("=" * 50)
        print(f"Generated at: {datetime.now()}")
        print()
        
        with self.Session() as session:
            # Legislature filter
            if legislatura_id:
                leg_filter = f"WHERE da.legislatura_id = {legislatura_id}"
                print(f" Legislature: {legislatura_id}")
            else:
                leg_filter = ""
                print(" Scope: All Legislatures")
            
            print()
            
            # Overall statistics
            total_deputies = session.execute(text(f"""
                SELECT COUNT(DISTINCT da.deputado_id) 
                FROM deputy_analytics da {leg_filter}
            """)).scalar() or 0
            
            total_records = session.execute(text(f"""
                SELECT COUNT(*) FROM deputy_analytics da {leg_filter}
            """)).scalar() or 0
            
            print(f" Total Deputies: {total_deputies}")
            print(f" Total Analytics Records: {total_records}")
            print()
            
            # Top performers
            print("TOP PERFORMERS TOP 10 PERFORMERS (by Activity Score)")
            print("-" * 40)
            
            top_performers = session.execute(text(f"""
                SELECT d.nome, da.activity_score, da.attendance_score, 
                       da.total_initiatives, da.total_interventions
                FROM deputy_analytics da
                JOIN deputados d ON da.deputado_id = d.id
                {leg_filter}
                ORDER BY da.activity_score DESC
                LIMIT 10
            """)).fetchall()
            
            for i, (nome, activity_score, attendance_score, initiatives, interventions) in enumerate(top_performers, 1):
                print(f"{i:2d}. {nome[:30]:<30} | Activity: {activity_score:3d} | Attendance: {attendance_score:3d} | Initiatives: {initiatives:3d} | Interventions: {interventions:3d}")
            
            print()
            
            # Data quality summary
            print(" DATA QUALITY SUMMARY")
            print("-" * 25)
            
            quality_summary = session.execute(text("""
                SELECT table_name, AVG(completeness_percentage) as avg_completeness,
                       AVG(consistency_score) as avg_consistency
                FROM data_quality_metrics
                WHERE metric_date >= DATE_SUB(NOW(), INTERVAL 7 DAY)
                GROUP BY table_name
                ORDER BY avg_completeness DESC
            """)).fetchall()
            
            for table_name, avg_completeness, avg_consistency in quality_summary:
                print(f"{table_name:<20} | Completeness: {avg_completeness:5.1f}% | Consistency: {avg_consistency:5.1f}")
            
            print()
    
    def _print_final_report(self):
        """Print final processing report."""
        print(" PROCESSING REPORT")
        print("=" * 30)
        print(f" Duration: {self.stats.duration}")
        print(f"  Legislatures Processed: {self.stats.legislatures_processed}")
        print(f" Deputies Processed: {self.stats.deputies_processed}")
        print(f" Records Created: {self.stats.analytics_records_created}")
        print(f" Records Updated: {self.stats.analytics_records_updated}")
        print(f" Success Rate: {self.stats.success_rate:.1f}%")
        
        if self.stats.errors:
            print(f" Errors: {len(self.stats.errors)}")
            if self.verbose:
                print("\nError Details:")
                for error in self.stats.errors[:10]:  # Show first 10 errors
                    print(f"   - {error}")
                if len(self.stats.errors) > 10:
                    print(f"   ... and {len(self.stats.errors) - 10} more errors")
        else:
            print(" No Errors")
        
        print()
    
    def _save_processing_log(self):
        """Save processing log to file."""
        log_data = {
            "timestamp": self.stats.start_time.isoformat(),
            "duration": self.stats.duration,
            "legislatures_processed": self.stats.legislatures_processed,
            "deputies_processed": self.stats.deputies_processed,
            "records_created": self.stats.analytics_records_created,
            "records_updated": self.stats.analytics_records_updated,
            "success_rate": self.stats.success_rate,
            "errors": self.stats.errors
        }
        
        # Create logs directory if it doesn't exist
        log_dir = os.path.join(os.path.dirname(__file__), "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        # Save log file
        log_filename = f"analytics_batch_{self.stats.start_time.strftime('%Y%m%d_%H%M%S')}.json"
        log_path = os.path.join(log_dir, log_filename)
        
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        print(f" Processing log saved to: {log_path}")


def main():
    """Main entry point for batch analytics processor."""
    parser = argparse.ArgumentParser(
        description='Batch Analytics Processor for Portuguese Parliament',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        '--legislature', '-l',
        type=int,
        help='Specific legislature ID to process'
    )
    
    parser.add_argument(
        '--schedule',
        choices=['daily', 'weekly', 'monthly'],
        help='Scheduled run type (affects logging and reporting)'
    )
    
    parser.add_argument(
        '--report-only', '-r',
        action='store_true',
        help='Generate analytics report without processing'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show verbose output with detailed progress'
    )
    
    args = parser.parse_args()
    
    try:
        # Get database connection
        engine = get_engine()
        
        # Create processor instance
        processor = BatchAnalyticsProcessor(engine, verbose=args.verbose)
        
        # Run processing
        processor.process_all_analytics(
            legislatura_id=args.legislature,
            report_only=args.report_only
        )
        
        if not args.report_only:
            print(" Batch analytics processing completed successfully!")
        
    except Exception as e:
        print(f" Error during batch processing: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()